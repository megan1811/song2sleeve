# ğŸµ Song2Sleeve

## ğŸ“˜ Introduction

**Song2Sleeve** is a Streamlit-based system for automatic **cover art generation** from an audio file. The core contribution of this work is a multi-modal conditioning pipeline that integrates **(1) lyric-level semantic features**, extracted via speech-to-text, and **(2) audio-driven descriptors** such as timbre, tempo, instrument tags, and instrument tags. This combined representation is used to condition a generative image model, enabling the creation of coherent, music-aware album artwork.

Album covers serve as a high-level visual summary of a trackâ€™s mood, style, and artistic identity. By unifying audio analysis with modern text-to-image models, Song2Sleeve demonstrates how cross-modal signals can enhance creative generation and yield more contextually aware artwork than lyric-only approaches.

## ğŸ•µğŸ»â€â™€ï¸ How it works

![Flowchart](assets/images/song2sleeve.drawio.png)

1. **Upload** a `.wav` audio file.
2. **Analyze** the song:
   - Transcribe lyrics with [Fasterâ€‘Whisper](https://github.com/guillaumekln/faster-whisper),
   - Extract tempo & spectral centroid (timbre) with [librosa](https://librosa.org),
   - Tag instruments with [YAMNet](https://tfhub.dev/google/yamnet/1).
3. **Generate a prompt** with [Claude 3 Sonnet](https://aws.amazon.com/bedrock/) using the outputs from stage 2.
4. **Create a cover** with [Stable Diffusion XL](https://aws.amazon.com/bedrock/), using the prompt generated from the previous stage.
5. **Output** A unique cover art image generated from extracted lyrics & audio elements.

## ğŸ¥ Tech Stack

- **Frontend:**: [**Streamlit**](https://streamlit.io) (UI, uploads, visualization).

- **Backend**: Python modular pipeline; on-instance inference with Demucs, Faster-Whisper, librosa, YAMNet (PyTorch/TF)

- **Cloud Inference**: on-instance inference with Demucs, Faster-Whisper, librosa, YAMNet (PyTorch/TF)

- **Deployment**: AWS EC2 (t3.medium) + optional GPU Spot; Docker + Poetry for reproducible environments

## ğŸ“¦ Local Installation

> âš ï¸ AWS Bedrock access is required for Claude & Stable Diffusion.  
> Set your AWS credentials (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`) in your environment.

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/megan1811/song2sleeve.git
cd song2sleeve
```

### 2ï¸âƒ£ Install dependencies

Using Poetry:

```bash
poetry install
poetry shell
```

Or using pip:

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run the app

```bash
streamlit run app.py
```

Open your browser at http://localhost:8501.

### 4ï¸âƒ£ Interface Example

âœ¨ \*Below is a snapshot of the app interface, after generating album art for the famous song **â€œHotel California.â€\*** by the Eagles. âœ¨

![Song2Sleeve Interface](assets/images/interface.png)

## ğŸ“‚ Project Structure

```bash
song2sleeve/
â”‚
â”œâ”€â”€ app.py            # Streamlit frontend
â”œâ”€â”€ pipeline.py       # High-level audio-to-image pipeline
â”œâ”€â”€ client.py         # AWS Bedrock client (Claude & SDXL)
â”œâ”€â”€ models.py         # Transcription (Whisper) & tagging (YAMNet)
â”œâ”€â”€ utils.py          # Feature extraction & prompt structuring
â”œâ”€â”€ poetry.toml       # Poetry dependency configuration
â””â”€â”€ README.md         # Project documentation
```

## ğŸ¾ Next Steps

A promising evolution of this project would be to develop an **audioâ€“image embedding model**, similar to [**CLIP**](https://github.com/openai/CLIP), that aligns audio representations directly with an image latent space. Unlike CLIPâ€™s textâ€“image contrastive setup, this approach would learn a joint space between audio features and visual concepts, allowing the model to capture musical and lyrical information end-to-end.

By bypassing the intermediate text-prompt stage, the system would preserve information often lost during transcription & instrument tagging as emotional tone and stylistic nuance. My hypothesis is that conditioning image generation on learned audio embeddings (rather than text prompts) would produce cover art that more faithfully reflects the input trackâ€™s artistic identity.
